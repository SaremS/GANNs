{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In order to model complex variable interactions, Neural Networks ingest all features at once, perform mostly black-box calculations and finally return a - hopefully accurate - output. This has obviously proven highly successful but mostly destroys any interpretability right from the start. In 'classic' Statistics, a lot of smart methods had been developed to model non-linear data without compromising for interpretable models. One of these techniques is known as **Generalized Additive Models** (GANs) and can be quite powerful. In today's post, I want to show how to marry Neural Networks with GANs - [not a particularly new invention](https://dl.acm.org/citation.cfm?id=312228) but I still liked the method so I decided to write this anyway. To provide some novelty, the demonstrations are done in [Julia](https://julialang.org/) this time - a language that I believe to be very powerful for many aspects of Data Science, Machine Learning and quantitative programming in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick primer on GAMs\n",
    "As the name implies, the most important aspect of GAMs is additivity, i.e. we want all its components to sum up to the final prediction. The standard formula for a (regression, the transfer to classification should be obvious) GAM model with $m$ variables is quite self-explanatory:  \n",
    "\n",
    "$$f_{GAM}(x)=\\alpha + f_1(x_1) + \\cdots + f_m(x_m).$$\n",
    "\n",
    "Instead of working with the raw features, we transform each variable through an according, often non-linear, function. The obvious problem here is of course the selection of the right basis functions $f_i$. While Spline basis functions (as e.g. in [MARS regression](https://numbersandcode.com/non-greedy-mars-regression)) has proven to be quite successful, we could basically choose any other transformation as well. If the size of the dataset is large enough, it could therefore make sense to learn the basis functions from the data as well and Neural Networks are a natural choice due to their inherent flexibility and adaptiveness.  \n",
    "\n",
    "In practice, instead of fitting a single large Neural Network for all features, we fit a smaller Neural Network for each feature according to the formula above. Alluding to the [performance bounds of Residual Neural Networks](https://papers.nips.cc/paper/7332-are-resnets-provably-better-than-linear-predictors.pdf), I decided to also include a linear block in the model, resulting in  \n",
    "  \n",
    "$$f_{ResidualGAM}(x)=\\alpha + f_1(x_1) + \\cdots + f_m(x_m) + X\\beta$$  \n",
    "  \n",
    "  \n",
    "where $X$ is the typical design matrix of all features as in the standard Linear Regression case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some thoughts on Julia\n",
    "At the moment, the main arguments pro Julia in general are definitely its [excellent performance](https://julialang.org/benchmarks/) and the fact that Julia packages are typically written in pure Julia alone. Another point why Julia could become awesome in the future, is the availability and ongoing development of sophisticaed AutoDiff packages like [Zygote.jl](https://github.com/FluxML/Zygote.jl) or [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl). Instead of writing complex pieces of code with Tensorflow's sometimes confusing API, AutoDiff in Julia typically means writing formulas in native Julia code and differentiating straight away. See for example [this video](https://www.youtube.com/watch?v=LjWzgTPFu14) for a nice overview on Zygote's capabilities.  \n",
    "\n",
    "On the flipside, Julia is evolving rapidly and hence things might often break or not work, even the latest examples from package docs. Also the functional programming paradigm could feel unfamiliar initially if you have only been working with Python so far - R users might actually have a slight edge here. If you are interested in learning the language, there are many sources available at [Julia's official webpage](https://julialang.org/learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "For this experiment, we'll use the [diamonds dataset](https://www.kaggle.com/shivam2503/diamonds) once again where we want to predict the price of diamond given some characteristic data. In order to perform the necessary data manipulations, I used Julias [Queryverse](https://github.com/queryverse/Queryverse.jl) framework which is inspired by the R tidyverse framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /Users/saremseitz/.julia/packages/CUDAdrv/3EzC1/src/CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Queryverse\n",
    "using MLJ\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>String</th><th>String</th><th>String</th><th>Float64</th><th>Float64</th><th>Int64</th><th>Float64</th></tr></thead><tbody><p>53,940 rows × 11 columns (omitted printing of 2 columns)</p><tr><th>1</th><td>1</td><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>2</td><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>3</td><td>0.23</td><td>Good</td><td>E</td><td>VS1</td><td>56.9</td><td>65.0</td><td>327</td><td>4.05</td></tr><tr><th>4</th><td>4</td><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>5</th><td>5</td><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>6</th><td>6</td><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>7</th><td>7</td><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>8</th><td>8</td><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>9</th><td>9</td><td>0.22</td><td>Fair</td><td>E</td><td>VS2</td><td>65.1</td><td>61.0</td><td>337</td><td>3.87</td></tr><tr><th>10</th><td>10</td><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>11</th><td>11</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>12</th><td>12</td><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>13</th><td>13</td><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>14</th><td>14</td><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>15</th><td>15</td><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>16</th><td>16</td><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>17</th><td>17</td><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>18</th><td>18</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>19</th><td>19</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>20</th><td>20</td><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>21</th><td>21</td><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>22</th><td>22</td><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>23</th><td>23</td><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>24</th><td>24</td><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>25</th><td>25</td><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>58.1</td><td>62.0</td><td>353</td><td>4.44</td></tr><tr><th>26</th><td>26</td><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>27</th><td>27</td><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>28</th><td>28</td><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>29</th><td>29</td><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>30</th><td>30</td><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & carat & cut & color & clarity & depth & table & price & x & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & String & String & String & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 3 & 0.23 & Good & E & VS1 & 56.9 & 65.0 & 327 & 4.05 & $\\dots$ \\\\\n",
       "\t4 & 4 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t5 & 5 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t6 & 6 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t7 & 7 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t8 & 8 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t9 & 9 & 0.22 & Fair & E & VS2 & 65.1 & 61.0 & 337 & 3.87 & $\\dots$ \\\\\n",
       "\t10 & 10 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t11 & 11 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t12 & 12 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t13 & 13 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t14 & 14 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t15 & 15 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t16 & 16 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t17 & 17 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t18 & 18 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t19 & 19 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t20 & 20 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t21 & 21 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t22 & 22 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t23 & 23 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t24 & 24 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t25 & 25 & 0.31 & Very Good & J & SI1 & 58.1 & 62.0 & 353 & 4.44 & $\\dots$ \\\\\n",
       "\t26 & 26 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t27 & 27 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t28 & 28 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t29 & 29 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t30 & 30 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "53940×11 DataFrame. Omitted printing of 4 columns\n",
       "│ Row   │ Column1 │ carat   │ cut       │ color  │ clarity │ depth   │ table   │\n",
       "│       │ \u001b[90mInt64\u001b[39m   │ \u001b[90mFloat64\u001b[39m │ \u001b[90mString\u001b[39m    │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m  │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├───────┼─────────┼─────────┼───────────┼────────┼─────────┼─────────┼─────────┤\n",
       "│ 1     │ 1       │ 0.23    │ Ideal     │ E      │ SI2     │ 61.5    │ 55.0    │\n",
       "│ 2     │ 2       │ 0.21    │ Premium   │ E      │ SI1     │ 59.8    │ 61.0    │\n",
       "│ 3     │ 3       │ 0.23    │ Good      │ E      │ VS1     │ 56.9    │ 65.0    │\n",
       "│ 4     │ 4       │ 0.29    │ Premium   │ I      │ VS2     │ 62.4    │ 58.0    │\n",
       "│ 5     │ 5       │ 0.31    │ Good      │ J      │ SI2     │ 63.3    │ 58.0    │\n",
       "│ 6     │ 6       │ 0.24    │ Very Good │ J      │ VVS2    │ 62.8    │ 57.0    │\n",
       "│ 7     │ 7       │ 0.24    │ Very Good │ I      │ VVS1    │ 62.3    │ 57.0    │\n",
       "│ 8     │ 8       │ 0.26    │ Very Good │ H      │ SI1     │ 61.9    │ 55.0    │\n",
       "│ 9     │ 9       │ 0.22    │ Fair      │ E      │ VS2     │ 65.1    │ 61.0    │\n",
       "│ 10    │ 10      │ 0.23    │ Very Good │ H      │ VS1     │ 59.4    │ 61.0    │\n",
       "⋮\n",
       "│ 53930 │ 53930   │ 0.71    │ Ideal     │ G      │ VS1     │ 61.4    │ 56.0    │\n",
       "│ 53931 │ 53931   │ 0.71    │ Premium   │ E      │ SI1     │ 60.5    │ 55.0    │\n",
       "│ 53932 │ 53932   │ 0.71    │ Premium   │ F      │ SI1     │ 59.8    │ 62.0    │\n",
       "│ 53933 │ 53933   │ 0.7     │ Very Good │ E      │ VS2     │ 60.5    │ 59.0    │\n",
       "│ 53934 │ 53934   │ 0.7     │ Very Good │ E      │ VS2     │ 61.2    │ 59.0    │\n",
       "│ 53935 │ 53935   │ 0.72    │ Premium   │ D      │ SI1     │ 62.7    │ 59.0    │\n",
       "│ 53936 │ 53936   │ 0.72    │ Ideal     │ D      │ SI1     │ 60.8    │ 57.0    │\n",
       "│ 53937 │ 53937   │ 0.72    │ Good      │ D      │ SI1     │ 63.1    │ 55.0    │\n",
       "│ 53938 │ 53938   │ 0.7     │ Very Good │ D      │ SI1     │ 62.8    │ 60.0    │\n",
       "│ 53939 │ 53939   │ 0.86    │ Premium   │ H      │ SI2     │ 61.0    │ 58.0    │\n",
       "│ 53940 │ 53940   │ 0.75    │ Ideal     │ D      │ SI2     │ 62.2    │ 55.0    │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.File(\"diamonds.csv\") |> DataFrame\n",
    "# the piping operator |> makes things feel like working with tidyverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "This part is quite simple - we only need to onehot-encode categorical data in order to make the algorithms work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940-element Array{Int64,1}:\n",
       "  326\n",
       "  326\n",
       "  327\n",
       "  334\n",
       "  335\n",
       "  336\n",
       "  336\n",
       "  337\n",
       "  337\n",
       "  338\n",
       "  339\n",
       "  340\n",
       "  342\n",
       "    ⋮\n",
       " 2756\n",
       " 2756\n",
       " 2756\n",
       " 2756\n",
       " 2757\n",
       " 2757\n",
       " 2757\n",
       " 2757\n",
       " 2757\n",
       " 2757\n",
       " 2757\n",
       " 2757"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num = convert(Matrix{Float32}, df |> @select(:carat, :depth, :table, :x, :y, :z)|> DataFrame)\n",
    "\n",
    "X_cut_dummy = Flux.onehotbatch(Array(df[:,3]), unique(Array(df[:,3]))) |> transpose\n",
    "X_color_dummy = Flux.onehotbatch(Array(df[:,4]), unique(Array(df[:,4]))) |> transpose\n",
    "X_clarity_dummy = Flux.onehotbatch(Array(df[:,5]), unique(Array(df[:,5]))) |> transpose\n",
    "\n",
    "X = hcat(X_num, X_cut_dummy, X_color_dummy, X_clarity_dummy)\n",
    "\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a GANN with Julia, Flux and Zygote\n",
    "When I first learned about Flux and Zyogte, I was impressed by how easy they make it to build customized Neural Network architectures and differentiable structures in general. For example, my (currently Github only) [experiments with Memory Augemented Neural Networks](https://github.com/SaremS/MANN) are written with Flux and Zygote's predecessor Tracker. As you can see below, we can build the whole architecture in about 25-30 lines of Julia code (excluding line-breaks, comments and additional functions). I also added some explanatory comments in case you are unfamiliar with Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GANN #Julia is a non-OOP language like C for example - hence, there are no classes\n",
    "    nets\n",
    "    W\n",
    "    b\n",
    "end\n",
    "Flux.@treelike(GANN) #This tells Flux to look for parameters in the GANN struct\n",
    "\n",
    "function GANN(n_features::Int, n_hidden::Int) \n",
    "    #we can simply use an array of Neural Networks (\"Chains\" in the Flux framwork) for the per-feature networks\n",
    "    nets = [Chain(Dense(1, n_hidden, relu), Dense(n_hidden, 1)) for i in 1:n_features] \n",
    "    W = Float32.((randn(n_features, 1)))\n",
    "    b = Float32.((randn(1)))\n",
    "    \n",
    "    GANN(nets, W, b)\n",
    "end\n",
    "\n",
    "#We split the per-feature network predictions and the linear predictions into two parts - this makes it\n",
    "#more convenient to infer the outputs later on\n",
    "function net_output(gann::GANN, X::Array)\n",
    "    n_variables = size(gann.nets)[1]\n",
    "    n_rows = size(X)[1]\n",
    "    net_output = [reshape(gann.nets[1](reshape(X[:,i], (1,n_rows))), (n_rows,1))\n",
    "                    for i in 1:n_variables]\n",
    "    \n",
    "    hcat(net_output...)\n",
    "end\n",
    "\n",
    "\n",
    "function linear_output(gann::GANN, X::Array)\n",
    "    X*gann.W .+ gann.b\n",
    "end\n",
    "\n",
    "#finally, we create a function that combines per-feature network and linear outputs\n",
    "function (m::GANN)(X::Array)\n",
    "    linear_output(m, X) .+ sum(net_output(m, X), dims = 2)\n",
    "end\n",
    "\n",
    "#If any functions change, we need to tell Zygote to update internally as well \n",
    "Flux.Zygote.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training\n",
    "Here, we perform a typical train-test split and re-scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455-element Array{Float32,1}:\n",
       " -0.99589497\n",
       " -0.99589497\n",
       " -0.9956654 \n",
       " -0.994058  \n",
       " -0.9938284 \n",
       " -0.9935988 \n",
       " -0.9935988 \n",
       " -0.99336916\n",
       " -0.99336916\n",
       " -0.99313956\n",
       " -0.9929099 \n",
       " -0.9926803 \n",
       " -0.99222106\n",
       "  ⋮         \n",
       " -0.8094439 \n",
       " -0.8094439 \n",
       " -0.8094439 \n",
       " -0.8092143 \n",
       " -0.8092143 \n",
       " -0.8092143 \n",
       " -0.8089847 \n",
       " -0.8089847 \n",
       " -0.8089847 \n",
       " -0.8089847 \n",
       " -0.8089847 \n",
       " -0.8089847 "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, test_idx = partition(eachindex(y), 0.75, shuffle = false)\n",
    "\n",
    "X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "y_train, y_test = (Float32.(y[train_idx])), (Float32.(y[test_idx]))\n",
    "\n",
    "\n",
    "X_train_m = X_train\n",
    "X_test_m = X_test\n",
    "\n",
    "\n",
    "mxs = [] # containers to store means and standard deviations of all exogenous features\n",
    "sxs = []\n",
    "for i in 1:11\n",
    "    mx = mean(X_train_m[:,i])\n",
    "    sx = std(X_train_m[:,i])\n",
    "   \n",
    "    X_train_m[:,i] .-= mx\n",
    "    X_test_m[:,i] .-= mx\n",
    "   \n",
    "    X_train_m[:,i] ./= sx\n",
    "    X_test_m[:,i] ./= sx\n",
    "\n",
    "    push!(mxs, mx)\n",
    "    push!(sxs, sx)\n",
    "       \n",
    "end\n",
    "\n",
    "my = mean(y_train[:,1])\n",
    "sy = std(y_train[:,1])\n",
    "\n",
    "y_train .-= my\n",
    "y_train ./= sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing model and optimizer\n",
    "This is quite standard. Since the training set is static, the loss function is also fixed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.1, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(345)\n",
    "gann = GANN(26, 10)\n",
    "\n",
    "mse_loss(x) = mean((gann(x) .- reshape(y_train, (40455, 1))).^2)\n",
    "#there are also built-in error functions in Flux\n",
    "\n",
    "Flux.Zygote.refresh()\n",
    "\n",
    "opt = Flux.ADAM(1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimization\n",
    "Also nothing too special here. Gradient calculation and parameter optimization are separated - this is rather personal preference than necessity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22934754\n",
      "0.09029856\n",
      "0.08533483\n",
      "0.08283875\n",
      "0.08104308\n",
      "0.07975853\n",
      "0.07886767\n",
      "0.07822754\n",
      "0.0777392\n",
      "0.07735237\n"
     ]
    }
   ],
   "source": [
    "ps = Flux.params(gann)\n",
    "\n",
    "for i in 1:400\n",
    "    loss, back = Flux.Zygote.pullback(()->mse_loss(X_train_m),ps)\n",
    "    grads = back(1.)\n",
    "    \n",
    "    if i%40==0; println(loss) end\n",
    "\n",
    "    Flux.Optimise.update!(opt,ps,grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the results\n",
    "Next, we want to see the marginal contribution of each variable like we would do for a plain linear model. Since the model is additive, we can simply cumulate the non-linear and linear effect of the single variables. Also keep in mind that missing important features or feature interactions might lead to unintuitive results. On the other hand, encountering strange effects like locally decreasing price for increasing diamond size could help us debugging the model or raise the need for additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_marginal_effect (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rescale(x, μ, σ)\n",
    "    x .* σ .+ μ\n",
    "end\n",
    "\n",
    "function plot_marginal_effect(ft::Int)\n",
    "    mn = minimum(X_train_m[:,ft])\n",
    "    mx = maximum(X_train_m[:,ft])\n",
    "\n",
    "    vals = collect(range(mn, length=1000, mx))[:,:]\n",
    "\n",
    "    linear_effect = gann.W[ft,1] .* vals[:,1]\n",
    "    nonlinear_effect = transpose(gann.nets[ft](reshape(vals[:,1],(1,1000))))\n",
    "\n",
    "    plot(rescale(vals[:,1], mxs[ft], sxs[ft]), rescale(linear_effect .+ nonlinear_effect, my, sy), lw=3,\n",
    "            label = \"Marginal Effect\")\n",
    "    xlabel!(\"Variable Value\")\n",
    "    ylabel!(\"Marginal Effect on Price\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip2100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2100)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2101\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2100)\" d=\"\n",
       "M349.806 1425.62 L2352.76 1425.62 L2352.76 47.2441 L349.806 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2102\">\n",
       "    <rect x=\"349\" y=\"47\" width=\"2004\" height=\"1379\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  660.86,1425.62 660.86,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1024.24,1425.62 1024.24,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1387.62,1425.62 1387.62,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1751,1425.62 1751,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2114.38,1425.62 2114.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,1371.31 2352.76,1371.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,1142.77 2352.76,1142.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,914.243 2352.76,914.243 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,685.712 2352.76,685.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,457.181 2352.76,457.181 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  349.806,228.65 2352.76,228.65 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,1425.62 349.806,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  660.86,1425.62 660.86,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1024.24,1425.62 1024.24,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1387.62,1425.62 1387.62,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1751,1425.62 1751,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2114.38,1425.62 2114.38,1404.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,1371.31 379.851,1371.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,1142.77 379.851,1142.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,914.243 379.851,914.243 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,685.712 379.851,685.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,457.181 379.851,457.181 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  349.806,228.65 379.851,228.65 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 660.86, 1479.62)\" x=\"660.86\" y=\"1479.62\">50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1024.24, 1479.62)\" x=\"1024.24\" y=\"1479.62\">60</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1387.62, 1479.62)\" x=\"1387.62\" y=\"1479.62\">70</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1751, 1479.62)\" x=\"1751\" y=\"1479.62\">80</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2114.38, 1479.62)\" x=\"2114.38\" y=\"1479.62\">90</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 1395.03)\" x=\"155.521\" y=\"1395.03\">5.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 1367.62)\" x=\"304.062\" y=\"1367.62\">3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 1166.5)\" x=\"155.521\" y=\"1166.5\">1.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 1139.09)\" x=\"304.062\" y=\"1139.09\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 937.971)\" x=\"155.521\" y=\"937.971\">1.5×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 910.561)\" x=\"304.062\" y=\"910.561\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 709.44)\" x=\"155.521\" y=\"709.44\">2.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 682.03)\" x=\"304.062\" y=\"682.03\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 480.909)\" x=\"155.521\" y=\"480.909\">2.5×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 453.498)\" x=\"304.062\" y=\"453.498\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 155.521, 252.378)\" x=\"155.521\" y=\"252.378\">3.0×10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:38px; text-anchor:start;\" transform=\"rotate(0, 304.062, 224.967)\" x=\"304.062\" y=\"224.967\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1351.28, 1559.48)\" x=\"1351.28\" y=\"1559.48\">Variable Value</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 736.431)\" x=\"89.2861\" y=\"736.431\">Marginal Effect on Price</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip2102)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  406.494,1348.15 408.385,1348.29 410.277,1348.42 412.168,1348.56 414.059,1348.7 415.951,1348.84 417.843,1348.98 419.734,1349.12 421.625,1349.25 423.517,1349.39 \n",
       "  425.408,1349.53 427.3,1349.67 429.191,1349.81 431.083,1349.95 432.974,1350.08 434.866,1350.22 436.757,1350.36 438.649,1350.5 440.54,1350.64 442.432,1350.78 \n",
       "  444.323,1350.91 446.215,1351.05 448.106,1351.19 449.997,1351.33 451.889,1351.47 453.78,1351.61 455.672,1351.74 457.563,1351.88 459.455,1352.02 461.346,1352.16 \n",
       "  463.238,1352.3 465.129,1352.44 467.021,1352.57 468.912,1352.71 470.803,1352.85 472.695,1352.99 474.586,1353.13 476.478,1353.27 478.369,1353.4 480.261,1353.54 \n",
       "  482.152,1353.68 484.044,1353.82 485.935,1353.96 487.827,1354.1 489.718,1354.23 491.61,1354.37 493.501,1354.51 495.393,1354.65 497.284,1354.79 499.176,1354.93 \n",
       "  501.067,1355.06 502.959,1355.2 504.85,1355.34 506.741,1355.48 508.633,1355.62 510.524,1355.76 512.416,1355.89 514.307,1356.03 516.199,1356.17 518.09,1356.31 \n",
       "  519.982,1356.45 521.873,1356.59 523.765,1356.72 525.656,1356.86 527.547,1357 529.439,1357.14 531.331,1357.28 533.222,1357.42 535.113,1357.56 537.005,1357.69 \n",
       "  538.896,1357.83 540.788,1357.97 542.679,1358.11 544.571,1358.25 546.462,1358.39 548.354,1358.52 550.245,1358.66 552.137,1358.8 554.028,1358.94 555.92,1359.08 \n",
       "  557.811,1359.22 559.703,1359.35 561.594,1359.49 563.485,1359.63 565.377,1359.77 567.268,1359.91 569.16,1360.05 571.051,1360.18 572.943,1360.32 574.834,1360.46 \n",
       "  576.726,1360.6 578.617,1360.74 580.509,1360.88 582.4,1361.01 584.291,1361.15 586.183,1361.29 588.075,1361.43 589.966,1361.57 591.857,1361.71 593.749,1361.84 \n",
       "  595.64,1361.98 597.532,1362.12 599.423,1362.26 601.315,1362.4 603.206,1362.54 605.098,1362.67 606.989,1362.81 608.881,1362.95 610.772,1363.09 612.664,1363.23 \n",
       "  614.555,1363.37 616.447,1363.5 618.338,1363.64 620.229,1363.78 622.121,1363.92 624.012,1364.06 625.904,1364.2 627.795,1364.33 629.687,1364.47 631.578,1364.61 \n",
       "  633.47,1364.75 635.361,1364.89 637.253,1365.03 639.144,1365.16 641.035,1365.3 642.927,1365.44 644.819,1365.58 646.71,1365.72 648.601,1365.86 650.493,1365.99 \n",
       "  652.384,1366.13 654.276,1366.27 656.167,1366.41 658.059,1366.55 659.95,1366.69 661.842,1366.82 663.733,1366.96 665.625,1367.1 667.516,1367.24 669.408,1367.38 \n",
       "  671.299,1367.52 673.19,1367.65 675.082,1367.79 676.973,1367.93 678.865,1368.07 680.756,1368.21 682.648,1368.35 684.539,1368.48 686.431,1368.62 688.322,1368.76 \n",
       "  690.214,1368.9 692.105,1369.04 693.997,1369.18 695.888,1369.31 697.779,1369.45 699.671,1369.59 701.562,1369.73 703.454,1369.87 705.345,1370.01 707.237,1370.14 \n",
       "  709.128,1370.28 711.02,1370.42 712.911,1370.56 714.803,1370.7 716.694,1370.84 718.586,1370.97 720.477,1371.11 722.369,1371.25 724.26,1371.39 726.151,1371.53 \n",
       "  728.043,1371.67 729.934,1371.8 731.826,1371.94 733.717,1372.08 735.609,1372.22 737.5,1372.36 739.392,1372.5 741.283,1372.63 743.175,1372.77 745.066,1372.91 \n",
       "  746.958,1373.05 748.849,1373.19 750.741,1373.33 752.632,1373.46 754.523,1373.6 756.415,1373.74 758.306,1373.88 760.198,1374.02 762.089,1374.16 763.981,1374.3 \n",
       "  765.872,1374.43 767.764,1374.57 769.655,1374.71 771.547,1374.85 773.438,1374.99 775.33,1375.13 777.221,1375.26 779.113,1375.4 781.004,1375.54 782.896,1375.68 \n",
       "  784.787,1375.82 786.678,1375.96 788.57,1376.09 790.461,1376.23 792.353,1376.37 794.244,1376.51 796.136,1376.65 798.027,1376.79 799.919,1376.92 801.81,1377.06 \n",
       "  803.702,1377.2 805.593,1377.34 807.485,1377.48 809.376,1377.62 811.267,1377.75 813.159,1377.89 815.05,1378.03 816.942,1378.17 818.833,1378.31 820.725,1378.45 \n",
       "  822.616,1378.58 824.508,1378.72 826.399,1378.86 828.291,1379 830.182,1379.14 832.074,1379.28 833.965,1379.41 835.857,1379.55 837.748,1379.69 839.64,1379.83 \n",
       "  841.531,1379.97 843.422,1380.11 845.314,1380.24 847.205,1380.38 849.097,1380.52 850.988,1380.66 852.88,1380.8 854.771,1380.94 856.663,1381.07 858.554,1381.21 \n",
       "  860.446,1381.35 862.337,1381.49 864.229,1381.63 866.12,1381.77 868.011,1381.9 869.903,1382.04 871.794,1382.18 873.686,1382.32 875.577,1382.46 877.469,1382.6 \n",
       "  879.36,1382.73 881.252,1382.87 883.143,1383.01 885.035,1383.15 886.926,1383.29 888.818,1383.43 890.709,1383.56 892.601,1383.7 894.492,1383.84 896.384,1383.98 \n",
       "  898.275,1384.12 900.166,1384.26 902.058,1384.39 903.949,1384.53 905.841,1384.67 907.732,1384.81 909.624,1384.95 911.515,1385.09 913.407,1385.22 915.298,1385.36 \n",
       "  917.19,1385.5 919.081,1385.64 920.973,1385.78 922.864,1385.92 924.756,1386.05 926.647,1386.19 928.538,1386.33 930.43,1386.47 932.321,1386.61 934.213,1386.12 \n",
       "  936.104,1384.31 937.996,1382.51 939.887,1380.7 941.779,1378.9 943.67,1377.09 945.562,1375.29 947.453,1373.48 949.345,1371.68 951.236,1369.87 953.128,1368.07 \n",
       "  955.019,1366.26 956.91,1364.45 958.802,1362.65 960.693,1360.84 962.585,1359.04 964.476,1357.23 966.368,1355.43 968.259,1353.62 970.151,1351.82 972.042,1350.01 \n",
       "  973.934,1348.21 975.825,1346.4 977.717,1344.6 979.608,1342.79 981.5,1340.98 983.391,1339.18 985.282,1337.37 987.174,1335.57 989.065,1333.76 990.957,1331.96 \n",
       "  992.848,1330.15 994.74,1328.35 996.631,1326.54 998.523,1324.74 1000.41,1322.93 1002.31,1321.13 1004.2,1319.32 1006.09,1317.51 1007.98,1315.71 1009.87,1313.9 \n",
       "  1011.76,1312.1 1013.65,1310.29 1015.55,1308.49 1017.44,1306.68 1019.33,1304.88 1021.22,1303.07 1023.11,1301.27 1025,1299.46 1026.89,1297.66 1028.79,1295.85 \n",
       "  1030.68,1294.04 1032.57,1292.24 1034.46,1290.43 1036.35,1288.63 1038.24,1286.82 1040.14,1285.02 1042.03,1283.21 1043.92,1281.41 1045.81,1279.6 1047.7,1277.8 \n",
       "  1049.59,1275.99 1051.48,1274.19 1053.38,1272.38 1055.27,1270.58 1057.16,1268.77 1059.05,1266.96 1060.94,1265.16 1062.83,1263.35 1064.72,1261.55 1066.62,1259.74 \n",
       "  1068.51,1257.94 1070.4,1256.13 1072.29,1254.33 1074.18,1252.52 1076.07,1250.72 1077.96,1248.91 1079.86,1247.11 1081.75,1245.3 1083.64,1243.49 1085.53,1241.69 \n",
       "  1087.42,1239.88 1089.31,1238.08 1091.2,1236.27 1093.1,1234.47 1094.99,1232.66 1096.88,1230.86 1098.77,1229.05 1100.66,1227.25 1102.55,1225.44 1104.44,1223.64 \n",
       "  1106.34,1221.83 1108.23,1220.02 1110.12,1218.22 1112.01,1216.41 1113.9,1214.61 1115.79,1212.8 1117.69,1211 1119.58,1209.19 1121.47,1207.39 1123.36,1205.58 \n",
       "  1125.25,1203.78 1127.14,1201.97 1129.03,1200.17 1130.93,1198.36 1132.82,1196.56 1134.71,1194.75 1136.6,1192.94 1138.49,1191.14 1140.38,1189.33 1142.27,1187.53 \n",
       "  1144.17,1185.72 1146.06,1183.92 1147.95,1182.11 1149.84,1180.31 1151.73,1178.5 1153.62,1176.7 1155.51,1174.89 1157.41,1173.09 1159.3,1171.28 1161.19,1169.47 \n",
       "  1163.08,1167.67 1164.97,1165.86 1166.86,1164.06 1168.75,1162.25 1170.65,1160.45 1172.54,1158.64 1174.43,1156.84 1176.32,1155.03 1178.21,1153.23 1180.1,1151.42 \n",
       "  1181.99,1149.62 1183.89,1147.81 1185.78,1146 1187.67,1144.2 1189.56,1142.39 1191.45,1140.59 1193.34,1138.78 1195.24,1136.98 1197.13,1135.17 1199.02,1133.37 \n",
       "  1200.91,1131.56 1202.8,1129.76 1204.69,1127.95 1206.58,1126.15 1208.48,1124.34 1210.37,1122.54 1212.26,1120.73 1214.15,1118.92 1216.04,1117.12 1217.93,1115.31 \n",
       "  1219.82,1113.51 1221.72,1111.7 1223.61,1109.9 1225.5,1108.09 1227.39,1106.29 1229.28,1104.48 1231.17,1102.68 1233.06,1100.87 1234.96,1099.07 1236.85,1097.26 \n",
       "  1238.74,1095.45 1240.63,1093.65 1242.52,1091.84 1244.41,1090.04 1246.3,1088.23 1248.2,1086.43 1250.09,1084.62 1251.98,1082.82 1253.87,1081.01 1255.76,1079.21 \n",
       "  1257.65,1077.4 1259.55,1075.6 1261.44,1073.79 1263.33,1071.98 1265.22,1070.18 1267.11,1068.37 1269,1066.57 1270.89,1064.76 1272.79,1062.96 1274.68,1061.15 \n",
       "  1276.57,1059.35 1278.46,1057.54 1280.35,1055.74 1282.24,1053.93 1284.13,1052.13 1286.03,1050.32 1287.92,1048.52 1289.81,1046.71 1291.7,1044.9 1293.59,1043.1 \n",
       "  1295.48,1041.29 1297.37,1039.49 1299.27,1037.68 1301.16,1035.88 1303.05,1034.07 1304.94,1032.27 1306.83,1030.46 1308.72,1028.66 1310.61,1026.85 1312.51,1025.05 \n",
       "  1314.4,1023.24 1316.29,1021.43 1318.18,1019.63 1320.07,1017.82 1321.96,1016.02 1323.85,1014.21 1325.75,1012.41 1327.64,1010.6 1329.53,1008.8 1331.42,1006.99 \n",
       "  1333.31,1005.19 1335.2,1003.38 1337.1,1001.58 1338.99,999.77 1340.88,997.965 1342.77,996.16 1344.66,994.354 1346.55,992.549 1348.44,990.743 1350.34,988.938 \n",
       "  1352.23,987.133 1354.12,985.327 1356.01,983.522 1357.9,981.717 1359.79,979.911 1361.68,978.106 1363.58,976.3 1365.47,974.495 1367.36,972.69 1369.25,970.884 \n",
       "  1371.14,969.079 1373.03,967.274 1374.92,965.468 1376.82,963.663 1378.71,961.858 1380.6,960.052 1382.49,958.247 1384.38,956.441 1386.27,954.636 1388.16,952.831 \n",
       "  1390.06,951.025 1391.95,949.22 1393.84,947.415 1395.73,945.609 1397.62,943.804 1399.51,941.998 1401.41,940.193 1403.3,938.388 1405.19,936.582 1407.08,934.777 \n",
       "  1408.97,932.972 1410.86,931.166 1412.75,929.361 1414.65,927.556 1416.54,925.75 1418.43,923.945 1420.32,922.139 1422.21,920.334 1424.1,918.529 1425.99,916.723 \n",
       "  1427.89,914.918 1429.78,913.113 1431.67,911.307 1433.56,909.502 1435.45,907.697 1437.34,905.891 1439.23,904.086 1441.13,902.28 1443.02,900.475 1444.91,898.67 \n",
       "  1446.8,896.864 1448.69,895.059 1450.58,893.254 1452.47,891.448 1454.37,889.643 1456.26,887.838 1458.15,886.032 1460.04,884.227 1461.93,882.421 1463.82,880.616 \n",
       "  1465.71,878.811 1467.61,877.005 1469.5,875.2 1471.39,873.395 1473.28,871.589 1475.17,869.784 1477.06,867.979 1478.96,866.173 1480.85,864.368 1482.74,862.562 \n",
       "  1484.63,860.757 1486.52,858.952 1488.41,857.146 1490.3,855.341 1492.2,853.536 1494.09,851.73 1495.98,849.925 1497.87,848.119 1499.76,846.314 1501.65,844.509 \n",
       "  1503.54,842.703 1505.44,840.898 1507.33,839.093 1509.22,837.287 1511.11,835.482 1513,833.677 1514.89,831.871 1516.78,830.066 1518.68,828.261 1520.57,826.455 \n",
       "  1522.46,824.65 1524.35,822.844 1526.24,821.039 1528.13,819.234 1530.02,817.428 1531.92,815.623 1533.81,813.817 1535.7,812.012 1537.59,810.207 1539.48,808.401 \n",
       "  1541.37,806.596 1543.26,804.791 1545.16,802.985 1547.05,801.18 1548.94,799.375 1550.83,797.569 1552.72,795.764 1554.61,793.958 1556.51,792.153 1558.4,790.348 \n",
       "  1560.29,788.542 1562.18,786.737 1564.07,784.932 1565.96,783.126 1567.85,781.321 1569.75,779.516 1571.64,777.71 1573.53,775.905 1575.42,774.099 1577.31,772.294 \n",
       "  1579.2,770.489 1581.09,768.683 1582.99,766.878 1584.88,765.073 1586.77,763.267 1588.66,761.462 1590.55,759.656 1592.44,757.851 1594.33,756.046 1596.23,754.24 \n",
       "  1598.12,752.435 1600.01,750.63 1601.9,748.824 1603.79,747.019 1605.68,745.214 1607.57,743.408 1609.47,741.603 1611.36,739.797 1613.25,737.992 1615.14,736.187 \n",
       "  1617.03,734.381 1618.92,732.576 1620.82,730.771 1622.71,728.965 1624.6,727.16 1626.49,725.355 1628.38,723.549 1630.27,721.744 1632.16,719.938 1634.06,718.133 \n",
       "  1635.95,716.328 1637.84,714.522 1639.73,712.717 1641.62,710.912 1643.51,709.106 1645.4,707.301 1647.3,705.495 1649.19,703.69 1651.08,701.885 1652.97,700.079 \n",
       "  1654.86,698.274 1656.75,696.469 1658.64,694.663 1660.54,692.858 1662.43,691.053 1664.32,689.247 1666.21,687.442 1668.1,685.636 1669.99,683.831 1671.88,682.026 \n",
       "  1673.78,680.22 1675.67,678.415 1677.56,676.61 1679.45,674.804 1681.34,672.999 1683.23,671.194 1685.13,669.388 1687.02,667.583 1688.91,665.777 1690.8,663.972 \n",
       "  1692.69,662.167 1694.58,660.361 1696.47,658.556 1698.37,656.751 1700.26,654.945 1702.15,653.14 1704.04,651.334 1705.93,649.529 1707.82,647.724 1709.71,645.918 \n",
       "  1711.61,644.113 1713.5,642.308 1715.39,640.502 1717.28,638.697 1719.17,636.892 1721.06,635.086 1722.95,633.281 1724.85,631.475 1726.74,629.67 1728.63,627.865 \n",
       "  1730.52,626.059 1732.41,624.254 1734.3,622.449 1736.19,620.643 1738.09,618.838 1739.98,617.033 1741.87,615.227 1743.76,613.422 1745.65,611.616 1747.54,609.811 \n",
       "  1749.43,608.006 1751.33,606.2 1753.22,604.395 1755.11,602.59 1757,600.784 1758.89,598.979 1760.78,597.173 1762.68,595.368 1764.57,593.563 1766.46,591.757 \n",
       "  1768.35,589.952 1770.24,588.147 1772.13,586.341 1774.02,584.536 1775.92,582.73 1777.81,580.925 1779.7,579.12 1781.59,577.314 1783.48,575.509 1785.37,573.704 \n",
       "  1787.26,571.898 1789.16,570.093 1791.05,568.288 1792.94,566.482 1794.83,564.677 1796.72,562.871 1798.61,561.066 1800.5,559.261 1802.4,557.455 1804.29,555.65 \n",
       "  1806.18,553.845 1808.07,552.039 1809.96,550.234 1811.85,548.429 1813.74,546.623 1815.64,544.818 1817.53,543.013 1819.42,541.207 1821.31,539.402 1823.2,537.596 \n",
       "  1825.09,535.791 1826.99,533.986 1828.88,532.18 1830.77,530.375 1832.66,528.57 1834.55,526.764 1836.44,524.959 1838.33,523.154 1840.23,521.348 1842.12,519.543 \n",
       "  1844.01,517.737 1845.9,515.932 1847.79,514.127 1849.68,512.321 1851.57,510.516 1853.47,508.711 1855.36,506.905 1857.25,505.1 1859.14,503.294 1861.03,501.489 \n",
       "  1862.92,499.684 1864.81,497.878 1866.71,496.073 1868.6,494.268 1870.49,492.462 1872.38,490.657 1874.27,488.851 1876.16,487.046 1878.05,485.241 1879.95,483.435 \n",
       "  1881.84,481.63 1883.73,479.825 1885.62,478.019 1887.51,476.214 1889.4,474.409 1891.29,472.603 1893.19,470.798 1895.08,468.992 1896.97,467.187 1898.86,465.382 \n",
       "  1900.75,463.576 1902.64,461.771 1904.54,459.966 1906.43,458.16 1908.32,456.355 1910.21,454.55 1912.1,452.744 1913.99,450.939 1915.88,449.134 1917.78,447.328 \n",
       "  1919.67,445.523 1921.56,443.717 1923.45,441.912 1925.34,440.107 1927.23,438.301 1929.12,436.496 1931.02,434.69 1932.91,432.885 1934.8,431.08 1936.69,429.274 \n",
       "  1938.58,427.469 1940.47,425.664 1942.36,423.858 1944.26,422.053 1946.15,420.248 1948.04,418.442 1949.93,416.637 1951.82,414.832 1953.71,413.026 1955.6,411.221 \n",
       "  1957.5,409.415 1959.39,407.61 1961.28,405.805 1963.17,403.999 1965.06,402.194 1966.95,400.388 1968.84,398.583 1970.74,396.778 1972.63,394.972 1974.52,393.167 \n",
       "  1976.41,391.362 1978.3,389.556 1980.19,387.751 1982.09,385.946 1983.98,384.14 1985.87,382.335 1987.76,380.529 1989.65,378.724 1991.54,376.919 1993.43,375.113 \n",
       "  1995.33,373.308 1997.22,371.503 1999.11,369.697 2001,367.892 2002.89,366.086 2004.78,364.281 2006.67,362.476 2008.57,360.67 2010.46,358.865 2012.35,357.06 \n",
       "  2014.24,355.254 2016.13,353.449 2018.02,351.644 2019.91,349.838 2021.81,348.033 2023.7,346.228 2025.59,344.422 2027.48,342.617 2029.37,340.811 2031.26,339.006 \n",
       "  2033.16,337.201 2035.05,335.395 2036.94,333.59 2038.83,331.785 2040.72,329.979 2042.61,328.174 2044.5,326.369 2046.4,324.563 2048.29,322.758 2050.18,320.952 \n",
       "  2052.07,319.147 2053.96,317.342 2055.85,315.536 2057.74,313.731 2059.64,311.926 2061.53,310.12 2063.42,308.315 2065.31,306.509 2067.2,304.704 2069.09,302.899 \n",
       "  2070.98,301.093 2072.88,299.288 2074.77,297.483 2076.66,295.677 2078.55,293.872 2080.44,292.067 2082.33,290.261 2084.22,288.456 2086.12,286.65 2088.01,284.845 \n",
       "  2089.9,283.04 2091.79,281.234 2093.68,279.429 2095.57,277.624 2097.46,275.818 2099.36,274.013 2101.25,272.208 2103.14,270.402 2105.03,268.597 2106.92,266.791 \n",
       "  2108.81,264.986 2110.7,263.181 2112.6,261.375 2114.49,259.57 2116.38,257.765 2118.27,255.959 2120.16,254.154 2122.05,252.349 2123.95,250.543 2125.84,248.738 \n",
       "  2127.73,246.932 2129.62,245.127 2131.51,243.322 2133.4,241.516 2135.29,239.711 2137.19,237.906 2139.08,236.1 2140.97,234.295 2142.86,232.49 2144.75,230.684 \n",
       "  2146.64,228.879 2148.53,227.073 2150.43,225.268 2152.32,223.463 2154.21,221.657 2156.1,219.852 2157.99,218.047 2159.88,216.241 2161.77,214.436 2163.67,212.63 \n",
       "  2165.56,210.825 2167.45,209.02 2169.34,207.215 2171.23,205.409 2173.12,203.604 2175.01,201.798 2176.91,199.993 2178.8,198.187 2180.69,196.382 2182.58,194.577 \n",
       "  2184.47,192.771 2186.36,190.966 2188.26,189.161 2190.15,187.355 2192.04,185.55 2193.93,183.745 2195.82,181.939 2197.71,180.134 2199.6,178.328 2201.5,176.523 \n",
       "  2203.39,174.718 2205.28,172.912 2207.17,171.107 2209.06,169.301 2210.95,167.496 2212.84,165.691 2214.74,163.885 2216.63,162.08 2218.52,160.275 2220.41,158.469 \n",
       "  2222.3,156.664 2224.19,154.859 2226.08,153.053 2227.98,151.248 2229.87,149.443 2231.76,147.637 2233.65,145.832 2235.54,144.026 2237.43,142.221 2239.32,140.416 \n",
       "  2241.22,138.61 2243.11,136.805 2245,135 2246.89,133.194 2248.78,131.389 2250.67,129.584 2252.56,127.778 2254.46,125.973 2256.35,124.167 2258.24,122.362 \n",
       "  2260.13,120.557 2262.02,118.751 2263.91,116.946 2265.81,115.141 2267.7,113.335 2269.59,111.53 2271.48,109.724 2273.37,107.919 2275.26,106.114 2277.15,104.308 \n",
       "  2279.05,102.503 2280.94,100.698 2282.83,98.8923 2284.72,97.0869 2286.61,95.2815 2288.5,93.4765 2290.39,91.6707 2292.29,89.8655 2294.18,88.0603 2296.07,86.2547 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2100)\" d=\"\n",
       "M1719.75 251.724 L2280.76 251.724 L2280.76 130.764 L1719.75 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1719.75,251.724 2280.76,251.724 2280.76,130.764 1719.75,130.764 1719.75,251.724 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2100)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1743.75,191.244 1887.75,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip2100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1911.75, 208.744)\" x=\"1911.75\" y=\"208.744\">Marginal Effect</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_marginal_effect(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the RMSE that our model produces on the test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791.3081f0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_gann = sqrt(mean((rescale(gann(X_test_m), my, sy).-y_test).^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison with a plain linear model \n",
    "Adding non-linearities to the model should considerably increase performance in comparison to a linear-only model. Otherwise, it would not make much sense to make the model overly complex and risk a performance decrease on out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13485-element Array{Float64,1}:\n",
       " -1.2107783843884141 \n",
       " -1.206658075772916  \n",
       " -1.2104818326065612 \n",
       " -1.213302372442071  \n",
       " -1.2026994966480613 \n",
       " -0.9165333374160093 \n",
       " -0.9937964451259677 \n",
       " -1.1370391674264897 \n",
       " -1.1475814672895348 \n",
       " -1.1370391674264897 \n",
       " -1.138697053197423  \n",
       " -1.1345762395116963 \n",
       " -0.9944770809812434 \n",
       "  ⋮                  \n",
       " -0.40726120268125204\n",
       " -0.3552963279933893 \n",
       " -0.04069043226788388\n",
       " -0.6135738229882306 \n",
       "  0.10760589467562953\n",
       "  0.10106178357843076\n",
       " -0.6520996125233193 \n",
       " -0.5918234533696183 \n",
       " -0.6720057970187654 \n",
       " -0.703548958046503  \n",
       " -1.0207233638955773 \n",
       " -1.025045027407605  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_b = hcat(X_train_m, ones(size(X_train_m)[1]))\n",
    "X_te_b = hcat(X_test_m, ones(size(X_test_m)[1]))\n",
    "\n",
    "β = inv(transpose(X_tr_b)*X_tr_b .+ 1e-4)*transpose(X_tr_b)*y_train\n",
    "\n",
    "lin_preds = X_te_b * β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the linear model performs worse than the GANN model. In fact, adding non-linearities almost halved the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484.2417363861507"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_linear = sqrt(mean((rescale(lin_preds, my, sy).-y_test).^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison with a Residual Neural Network\n",
    "For completeness sake, let's also compare the results with a Neural Network architecture that uses all input features at once. Here, I decided to use a one-layer ResNet as the topology is closer to the GANN above than a standard Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ResNet\n",
    "    net\n",
    "    W\n",
    "    b\n",
    "end\n",
    "Flux.@treelike(ResNet)\n",
    "\n",
    "\n",
    "function ResNet(n_features::Int, n_hidden::Int) \n",
    "    net = Chain(Dense(n_features, n_hidden, relu), Dense(n_hidden, 1))\n",
    "    W = Float32.((randn(n_features, 1)))\n",
    "    b = Float32.((randn(1)))\n",
    "    \n",
    "    ResNet(net, W, b)\n",
    "end\n",
    "\n",
    "\n",
    "function net_output(resn::ResNet, X::Array)\n",
    "    n_rows, n_variables = size(X)\n",
    "    \n",
    "    rnet.net(transpose(X))[1,:]\n",
    "end\n",
    "\n",
    "\n",
    "function linear_output(resn::ResNet, X::Array)\n",
    "    (X*resn.W .+ resn.b)[:,1]\n",
    "end\n",
    "\n",
    "\n",
    "function (m::ResNet)(X::Array)\n",
    "    net_output(m, X) + linear_output(m, X)\n",
    "end\n",
    "\n",
    "Flux.Zygote.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(Chain(Dense(26, 10, relu), Dense(10, 1)), Float32[1.1799979; -0.12719862; … ; -0.13458526; -1.6977073], Float32[0.6811983])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(345)\n",
    "rnet = ResNet(26, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09988803\n",
      "0.064022504\n",
      "0.040490765\n",
      "0.029970031\n",
      "0.027306134\n",
      "0.026876403\n",
      "0.026769876\n",
      "0.026694845\n",
      "0.026632553\n",
      "0.026578257\n"
     ]
    }
   ],
   "source": [
    "mse_loss_rnet(x) = mean((rnet(x) .- y_train).^2)\n",
    "Flux.Zygote.refresh()\n",
    "\n",
    "opt = Flux.ADAM(1e-1)\n",
    "\n",
    "ps = Flux.params(rnet)\n",
    "\n",
    "for i in 1:400\n",
    "    loss, back = Flux.Zygote.pullback(()->mse_loss_rnet(X_train_m),ps)\n",
    "    grads = back(1.)\n",
    "    \n",
    "    if i%40==0; println(loss) end\n",
    "\n",
    "    Flux.Optimise.update!(opt,ps,grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the GANN model is beaten by the ResNet by a large margin again. We could likely reduce this gap by accounting for variable interactions in the GANN model which was completely ignored so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416.54053f0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_rnet = sqrt(mean((rescale(rnet(X_test_m), my, sy).-y_test).^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "There is still a lot of room for improvement - a resonable next step could be adding rule-features from Decision Tree models in a [RuleFit](https://numbersandcode.com/rulefit-interpretable-machine-learning) manner which might ideally get us closer to ResNet.  \n",
    "\n",
    "An important question now is why even bother with fully interpretable models, when methods like [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) could easily give us post-hoc explanations of a black-box ResNet model. However, there are some arguments against this obvious solution:\n",
    "\n",
    "1. [Post-hoc explanatations can fall victim to adversarial attacks](https://arxiv.org/pdf/1911.02508.pdf)\n",
    "2. Interpretable models allow us to naturally incorporate and utilize human prior knowledge\n",
    "3. Linear models allow us to easily perform statistical inference \n",
    "4. At the moment, mathematical properties of linear models are much better researched than black-boxes although this might change in the future\n",
    "\n",
    "Hence, interpretable models models should still be considered as a valid alternative to complex ML solutions. There are still many ideas to explore and future developments on both ends - performant white-boxes and explainable black-boxes - will likely be very exciting over the next few years.  "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
